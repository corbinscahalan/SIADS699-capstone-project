{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Data Extraction and Cleaning\n",
        "This notebook demonstrates how to extract video data from the YouTube API using two methods:\n",
        "<ol>\n",
        "  <li>Scraped Channels: Scrape channels based on article that list popular cooking channels, then plug those channels into the YouTube API to collect each channel's video data.</li>\n",
        "  <li>Search Based: Plug in a cooking keyword directly into the YouTube API to collect the highest ranked videos per keyword searched on YouTube.</li>\n",
        "</ol>\n",
        "\n",
        "The first method (scraped channels) guarantees that generally agreed upon popular cooking YouTube channels are in our dataset, whereas the second method (search based) collects the highest ranked videos based on cooking terms. We believe that these were the best two methods to attempt to collect cooking videos for our dataset."
      ],
      "metadata": {
        "id": "hIaMyxeDdD10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages\n",
        "These are the imports needed to run data extraction and cleaning."
      ],
      "metadata": {
        "id": "zXjVr6RZef1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Lj32P-TEnjfX"
      },
      "outputs": [],
      "source": [
        "#general\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#website scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "#youtube api\n",
        "import googleapiclient.discovery\n",
        "from googleapiclient.discovery import build\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "#data cleaning\n",
        "import isodate\n",
        "\n",
        "#api utls\n",
        "from typing import List, Set, Dict, Tuple, Optional\n",
        "from sklearn import linear_model\n",
        "import os\n",
        "from googleapiclient.errors import HttpError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YouTube API Setup\n",
        "First, get a YouTube API key so that you can extract video data. The function below creates a YouTube API client."
      ],
      "metadata": {
        "id": "a3V4zB_GdBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual API key to extract videos via web scraping method. \n",
        "# As of Nov 2022, you shouldn't need more than one API key if you are scraping <50 websites.\n",
        "\n",
        "api_key = '' "
      ],
      "metadata": {
        "id": "TFVT3ENfc_6D"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API List to extract videos via search based method.\n",
        "# Search based method is expensive in terms of API requests.\n",
        "\n",
        "api_list = [''\n",
        "            '',\n",
        "            ''\n",
        "            ]"
      ],
      "metadata": {
        "id": "IYgCbLejuTns"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_client(api_key: str) -> object:\n",
        "\n",
        "    # Creates a YouTube API client for use in subsequent requests\n",
        "    # User's API key is only needed once to create the client\n",
        "\n",
        "    yt_client = googleapiclient.discovery.build('youtube', 'v3', developerKey = api_key)\n",
        "\n",
        "    return yt_client"
      ],
      "metadata": {
        "id": "Fpy22O1Pf8RM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method #1: Scraped Channels\n",
        "This method is used to collect publicly agreed upon \"popular\" cooking channels. Note that most articles don't really define popularity, but it seems that the articles rank the most \"popular\" channels based on each channel's total view count or total subscriber count."
      ],
      "metadata": {
        "id": "1OHUlLmogH-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Scraper"
      ],
      "metadata": {
        "id": "O3eMN1_sg_a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_channel_ids(url, id_type):\n",
        "\n",
        "    # Scrape channel IDs and channel usernames from YouTube URLs on any given website. \n",
        "    # We used ~50 websites that recommended top cooking channels\n",
        "\n",
        "    # Parameters:\n",
        "    # url: website URL as a string\n",
        "    # id_type: the ID type you want to scrape; 'channel_id' or 'channel_username'\n",
        "\n",
        "    page = requests.get(url) \n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    urls = soup.find_all('a', href=True)\n",
        "\n",
        "    hrefs=[]\n",
        "    for item in urls:\n",
        "        hrefs.append(item.get('href'))\n",
        "\n",
        "    youtube_urls=[]\n",
        "    for item in hrefs:\n",
        "        if id_type == 'channel_id':\n",
        "            youtube_urls.append(re.findall(\"https://www.youtube.com/channel[^\\s?]+\", item))\n",
        "        if id_type == 'channel_username':\n",
        "            youtube_urls.append(re.findall(\"https://www.youtube.com/c/[^\\s?]+\", item))\n",
        "\n",
        "\n",
        "    flat_list = [str(item.split('/')[4]) for sublist in youtube_urls for item in sublist] #just the channel ids\n",
        "\n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "KC2CNdj1gCFV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Websites\n",
        "In this demo, we only scrape 5 websites, but we scraped roughly 45 websites for channel IDs and usernames. We've provided the full list of website URLs below."
      ],
      "metadata": {
        "id": "o5nQZLWsiXcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full list of URLs used for project scraping\n",
        "urls = ['https://www.equipmentnerd.com/top-food-cooking-youtubers-list/',\n",
        "        'https://www.purewow.com/food/best-cooking-channels-on-youtube',\n",
        "        'https://www.thrillist.com/entertainment/nation/best-youtube-cooking-channels',\n",
        "        'https://www.cinemablend.com/television/2561526/great-cooking-channels-to-subscribe-to-on-youtube',\n",
        "        'https://techboomers.com/best-youtube-cooking-channels',\n",
        "        'https://www.businessinsider.com/youtube-best-cooking-channels-2017-11',\n",
        "        'https://www.mob.co.uk/life/best-youtube-food-channels',\n",
        "        'https://www.netinfluencer.com/top-cooking-channels-on-youtube/',\n",
        "        'https://www.spark-lang.com/blog/5-great-cooking-youtube-channels-for-english-learners',\n",
        "        'https://www.cleaneatingkitchen.com/best-youtube-cooking-channels/',\n",
        "        'https://www.finedininglovers.com/article/best-youtube-food-channels',\n",
        "        'https://mashable.com/article/best-youtube-cooking-channels',\n",
        "        'https://www.thekitchn.com/youtube-most-popular-cooking-channels-258119',\n",
        "        'https://www.mashed.com/146555/ranking-the-most-popular-cooking-channels-on-youtube-from-best-to-worst/',\n",
        "        'https://camp.com/cooking/cooking-youtube-channels-for-kids',\n",
        "        'https://www.starterstory.com/cooking-youtube-channels',\n",
        "        'https://www.scrolldroll.com/best-indian-cooking-channels-on-youtube/',\n",
        "        'https://www.kobejones.com.au/top-5-youtube-channels-for-japanese-food-lovers/',\n",
        "        'https://sprintkitchen.com/youtube-cooking-channels/',\n",
        "        'https://purgula.com/kitchen/best-international-cooking-channels-on-youtube/',\n",
        "        'https://brentwoodnylibrary.org/adult/history-food-some-historical-cooking-channels-youtube',\n",
        "        'https://www.foodbeast.com/news/top-youtube-chefs-worth-clicking-subscribe-for/',\n",
        "        'https://www.gadgetbridge.com/gadget-bridge-ace/best-youtube-channels-to-follow-for-recipes-and-home-cooking/',\n",
        "        'https://foodzodiac.com/best-indian-cooking-channels-on-youtube/',\n",
        "        'https://medium.com/creative-landscape-of-youtube/5-awesome-food-shows-on-youtube-that-cook-things-a-bit-differently-814b52ea7355',\n",
        "        'https://techbanta.com/article/best-youtube-cooking-channels/',\n",
        "        'https://www.shortform.com/blog/best-cooking-blogs-podcasts-youtube-channels/',\n",
        "        'https://spoonuniversity.com/lifestyle/quick-and-easy-recipes-on-youtube',\n",
        "        'https://www.foodforfitness.co.uk/best-youtube-recipes/',\n",
        "        'https://blog.feedspot.com/food_youtube_channels/',\n",
        "        'https://hiplatina.com/latin-cooking-channels-tutorials-videos/',\n",
        "        'https://blog.feedspot.com/home_cooking_youtube_channels/',\n",
        "        'https://blog.feedspot.com/italian_food_youtube_channels/',\n",
        "        'https://blog.feedspot.com/japanese_food_youtube_channels/',\n",
        "        'https://blog.feedspot.com/chinese_food_youtube_channels/',\n",
        "        'https://blog.feedspot.com/indian_food_youtube_channels/',\n",
        "        'https://blog.feedspot.com/asian_food_youtube_channels/',\n",
        "        'https://www.businessinsider.com/the-best-youtube-cooking-channels-2019-10',\n",
        "        'https://blog.feedspot.com/baking_youtube_channels/',\n",
        "        'https://blog.feedspot.com/bbq_youtube_channels/',\n",
        "        'https://blog.feedspot.com/vegetarian_youtube_channels/',\n",
        "        'https://blog.feedspot.com/vegan_food_youtube_channels/',\n",
        "        'https://blog.feedspot.com/gluten_free_youtube_channels/',\n",
        "        'https://blog.feedspot.com/healthy_food_youtube_channels/',\n",
        "        'https://blog.feedspot.com/street_food_youtube_channels/']"
      ],
      "metadata": {
        "id": "spCO55n1im9q"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scrape the first 5 URLs for the demo\n",
        "urls = urls[0:5]"
      ],
      "metadata": {
        "id": "7IQu0u90pfC7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the channel IDs and channel usernames from the URLs\n",
        "\n",
        "channel_ids_lists = []\n",
        "channel_username_lists = []\n",
        "\n",
        "for url in urls:\n",
        "  channel_ids_lists.append(scrape_channel_ids(url, 'channel_id'))\n",
        "  channel_username_lists.append(scrape_channel_ids(url, 'channel_username'))\n",
        "\n",
        "all_channel_ids = [item for sublist in channel_ids_lists for item in sublist]\n",
        "all_channel_usernames = [item for sublist in channel_username_lists for item in sublist]\n",
        "\n",
        "unique_channel_ids = list(dict.fromkeys(all_channel_ids))\n",
        "unique_channel_usernames = list(dict.fromkeys(all_channel_usernames))"
      ],
      "metadata": {
        "id": "EjE_m8M_izZQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Channel IDs:\",len(unique_channel_ids))\n",
        "print(\"Unique Channel Usernames:\", len(unique_channel_usernames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m3x9fHfjFxi",
        "outputId": "a12233d8-3781-4600-8b85-673847d737c8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Channel IDs: 64\n",
            "Unique Channel Usernames: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video Extraction Functions for YouTube API"
      ],
      "metadata": {
        "id": "GUbJl_SVhz0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_channel_stats(youtube, channel_id, id_type):\n",
        "    \n",
        "    # Get the YouTube API response based on the IDs you scrape using scrape_channel_ids\n",
        "\n",
        "    # Parameters:\n",
        "    # youtube: result of make_client\n",
        "    # channel_id: scraped ID\n",
        "    # id_type: 'channel_id' or 'channel_username'\n",
        "\n",
        "    if id_type == 'channel_id':\n",
        "        request = youtube.channels().list(\n",
        "            part = 'snippet,contentDetails,statistics',\n",
        "            id=channel_id\n",
        "        )\n",
        "\n",
        "        response = request.execute()\n",
        "  \n",
        "    if id_type == 'channel_username':\n",
        "        request = youtube.channels().list(\n",
        "        part = 'snippet,contentDetails,statistics',\n",
        "        forUsername=channel_id\n",
        "        )\n",
        "\n",
        "        response = request.execute()\n",
        "    \n",
        "    return response['items']\n",
        "\n",
        "def get_video_list(youtube, upload_id):\n",
        "\n",
        "    # Get a channel's videos based on the upload id\n",
        "\n",
        "    # Parameters:\n",
        "    # youtube: result of make_client\n",
        "    # upload_id:  channel_stats[0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "    video_list = []\n",
        "    request = youtube.playlistItems().list(\n",
        "        part=\"snippet,contentDetails\",\n",
        "        playlistId=upload_id,\n",
        "        maxResults=50\n",
        "    )\n",
        "    next_page = True\n",
        "    while next_page:\n",
        "        response = request.execute()\n",
        "        data = response['items']\n",
        "\n",
        "        for video in data:\n",
        "            video_id = video['contentDetails']['videoId']\n",
        "            if video_id not in video_list:\n",
        "                video_list.append(video_id)\n",
        "\n",
        "        if 'nextPageToken' in response.keys():\n",
        "            next_page = True\n",
        "            request = youtube.playlistItems().list(\n",
        "                part=\"snippet,contentDetails\",\n",
        "                playlistId=upload_id,\n",
        "                pageToken=response['nextPageToken'],\n",
        "                maxResults=50\n",
        "            )\n",
        "        else:\n",
        "            next_page = False\n",
        "\n",
        "    return video_list\n",
        "\n",
        "def get_video_details(youtube, video_list):\n",
        "\n",
        "    # Extract the columns needed for the final data frame based on the video list\n",
        "\n",
        "    # Parameters:\n",
        "    # youtube: result of make_client\n",
        "    # video_list: videos from get_video_list\n",
        "\n",
        "    stats_list=[]\n",
        "    for i in range(0, len(video_list), 50):\n",
        "        request= youtube.videos().list(\n",
        "            part=\"snippet,contentDetails,statistics\",\n",
        "            id=video_list[i:i+50]\n",
        "        )\n",
        "\n",
        "        data = request.execute()\n",
        "        for video in data['items']:\n",
        "            chan_id = video['snippet']['channelId']\n",
        "            vid_id = video['id']\n",
        "            vid_name = video['snippet']['title']\n",
        "            vid_publish_dt = video['snippet']['publishedAt']\n",
        "            vid_thumb = video['snippet']['thumbnails']['default']['url']\n",
        "            vid_duration = video['contentDetails']['duration']\n",
        "            vid_caption = video['contentDetails']['caption']\n",
        "            vid_viewcount = video['statistics'].get('viewCount',0)\n",
        "            vid_likecount = video['statistics'].get('likeCount',0)\n",
        "            vid_commentcount = video['statistics'].get('commentCount',0)\n",
        "            data_dict=dict(chan_id=chan_id, vid_id=vid_id, vid_name=vid_name, vid_publish_dt=vid_publish_dt,\n",
        "                          vid_thumb=vid_thumb,vid_duration=vid_duration,vid_caption=vid_caption,vid_viewcount=vid_viewcount,\n",
        "                          vid_likecount=vid_likecount,vid_commentcount=vid_commentcount)\n",
        "            stats_list.append(data_dict)\n",
        "\n",
        "    return stats_list\n",
        "\n",
        "def create_video_df(youtube, channel_ids, id_type):\n",
        "\n",
        "    # Input a list of channel IDs, and this function outputs all videos for those channel IDs in the format needed for this project\n",
        "\n",
        "    # Parameters:\n",
        "    # youtube: result of make_client\n",
        "    # channel_ids: list of channel IDs\n",
        "    # id_type: 'channel_id' or 'channel_username' \n",
        "\n",
        "    channel_dfs = []\n",
        "    vid_dfs = []\n",
        "    for channel_id in channel_ids:\n",
        "        try:\n",
        "            channel_stats = get_channel_stats(youtube, channel_id, id_type)\n",
        "\n",
        "            channel_dfs.append(pd.json_normalize(channel_stats))\n",
        "\n",
        "            upload_id = channel_stats[0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "            video_list = get_video_list(youtube, upload_id)\n",
        "\n",
        "            video_data = get_video_details(youtube, video_list)\n",
        "            vid_dfs.append(pd.json_normalize(video_data))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    channel_df = pd.concat(channel_dfs)\n",
        "\n",
        "    vid_df = pd.concat(vid_dfs)\n",
        "\n",
        "    channel_df = channel_df.rename(columns={'id':'chan_id','snippet.title':'chan_name','statistics.viewCount':'chan_viewcount',\n",
        "                                            'statistics.subscriberCount':'chan_subcount','snippet.publishedAt':'chan_start_dt',\n",
        "                                            'snippet.thumbnails.default.url':'chan_thumb','statistics.videoCount':'chan_vidcount'})\n",
        "  \n",
        "    channel_df = channel_df[['chan_id','chan_name','chan_viewcount','chan_subcount','chan_start_dt','chan_thumb','chan_vidcount']]\n",
        "\n",
        "    final_df = vid_df.merge(channel_df, how='left', on='chan_id')\n",
        "\n",
        "    column_order = ['chan_id','chan_name','chan_viewcount','chan_subcount','chan_start_dt','chan_thumb','chan_vidcount',\n",
        "                    'vid_id','vid_name','vid_publish_dt','vid_thumb','vid_duration','vid_caption','vid_viewcount','vid_likecount','vid_commentcount']\n",
        "    \n",
        "    return final_df[column_order]\n"
      ],
      "metadata": {
        "id": "9jSyZlSyghwM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Videos from YouTube API\n",
        "For this demo, we'll just use the channel ID list. This will give us enough videos."
      ],
      "metadata": {
        "id": "zv8L3fVajitE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube API Client\n",
        "youtube = make_client(api_key)"
      ],
      "metadata": {
        "id": "pCl_wn8ujiFx"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vids_by_channel_id = create_video_df(youtube, all_channel_ids, 'channel_id')"
      ],
      "metadata": {
        "id": "fnnsoQUmiHar"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vids_by_channel_id.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "VP5bX3lblG2Q",
        "outputId": "0e0bcba0-bcbd-4ff0-e19c-9fa46139e352"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    chan_id       chan_name chan_viewcount chan_subcount  \\\n",
              "0  UCYjk_zY-iYR8YNfJmuzd70A  Epic Meal Time      562726381       6890000   \n",
              "1  UCYjk_zY-iYR8YNfJmuzd70A  Epic Meal Time      562726381       6890000   \n",
              "2  UCYjk_zY-iYR8YNfJmuzd70A  Epic Meal Time      562726381       6890000   \n",
              "3  UCYjk_zY-iYR8YNfJmuzd70A  Epic Meal Time      562726381       6890000   \n",
              "4  UCYjk_zY-iYR8YNfJmuzd70A  Epic Meal Time      562726381       6890000   \n",
              "\n",
              "          chan_start_dt                                         chan_thumb  \\\n",
              "0  2010-09-29T19:34:09Z  https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...   \n",
              "1  2010-09-29T19:34:09Z  https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...   \n",
              "2  2010-09-29T19:34:09Z  https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...   \n",
              "3  2010-09-29T19:34:09Z  https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...   \n",
              "4  2010-09-29T19:34:09Z  https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...   \n",
              "\n",
              "  chan_vidcount       vid_id  \\\n",
              "0           428  7zmtAqjWmOo   \n",
              "1           428  WHWc-l5ZpPk   \n",
              "2           428  M5I7kBjPv7o   \n",
              "3           428  e9zp3jQy-98   \n",
              "4           428  Mdn687guoB8   \n",
              "\n",
              "                                            vid_name        vid_publish_dt  \\\n",
              "0       Do I Tip when I Eat Out? | Binge Eater Ep.10  2022-11-28T19:00:03Z   \n",
              "1                  WORLD’s FIRST CANDIED APPLE PIE!!  2022-11-26T16:45:00Z   \n",
              "2       Never Trust an Influencer | Binge Eater Ep.9  2022-11-21T21:25:58Z   \n",
              "3  Rejecting $100,000 He Will Never Get Back | Bi...  2022-11-14T21:11:28Z   \n",
              "4                      WATCH YOUR LANGUAGE!! #shorts  2022-11-08T16:41:00Z   \n",
              "\n",
              "                                        vid_thumb vid_duration vid_caption  \\\n",
              "0  https://i.ytimg.com/vi/7zmtAqjWmOo/default.jpg     PT58M53S       false   \n",
              "1  https://i.ytimg.com/vi/WHWc-l5ZpPk/default.jpg      PT6M21S       false   \n",
              "2  https://i.ytimg.com/vi/M5I7kBjPv7o/default.jpg   PT1H19M17S       false   \n",
              "3  https://i.ytimg.com/vi/e9zp3jQy-98/default.jpg   PT1H23M14S       false   \n",
              "4  https://i.ytimg.com/vi/Mdn687guoB8/default.jpg        PT54S       false   \n",
              "\n",
              "  vid_viewcount vid_likecount vid_commentcount  \n",
              "0          3887           148               66  \n",
              "1         15724          1004              236  \n",
              "2          7508           272               58  \n",
              "3          7891           266               75  \n",
              "4         23751          1122               24  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11fb3046-e24d-47b1-a6e2-77a7dc51b350\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chan_id</th>\n",
              "      <th>chan_name</th>\n",
              "      <th>chan_viewcount</th>\n",
              "      <th>chan_subcount</th>\n",
              "      <th>chan_start_dt</th>\n",
              "      <th>chan_thumb</th>\n",
              "      <th>chan_vidcount</th>\n",
              "      <th>vid_id</th>\n",
              "      <th>vid_name</th>\n",
              "      <th>vid_publish_dt</th>\n",
              "      <th>vid_thumb</th>\n",
              "      <th>vid_duration</th>\n",
              "      <th>vid_caption</th>\n",
              "      <th>vid_viewcount</th>\n",
              "      <th>vid_likecount</th>\n",
              "      <th>vid_commentcount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UCYjk_zY-iYR8YNfJmuzd70A</td>\n",
              "      <td>Epic Meal Time</td>\n",
              "      <td>562726381</td>\n",
              "      <td>6890000</td>\n",
              "      <td>2010-09-29T19:34:09Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...</td>\n",
              "      <td>428</td>\n",
              "      <td>7zmtAqjWmOo</td>\n",
              "      <td>Do I Tip when I Eat Out? | Binge Eater Ep.10</td>\n",
              "      <td>2022-11-28T19:00:03Z</td>\n",
              "      <td>https://i.ytimg.com/vi/7zmtAqjWmOo/default.jpg</td>\n",
              "      <td>PT58M53S</td>\n",
              "      <td>false</td>\n",
              "      <td>3887</td>\n",
              "      <td>148</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UCYjk_zY-iYR8YNfJmuzd70A</td>\n",
              "      <td>Epic Meal Time</td>\n",
              "      <td>562726381</td>\n",
              "      <td>6890000</td>\n",
              "      <td>2010-09-29T19:34:09Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...</td>\n",
              "      <td>428</td>\n",
              "      <td>WHWc-l5ZpPk</td>\n",
              "      <td>WORLD’s FIRST CANDIED APPLE PIE!!</td>\n",
              "      <td>2022-11-26T16:45:00Z</td>\n",
              "      <td>https://i.ytimg.com/vi/WHWc-l5ZpPk/default.jpg</td>\n",
              "      <td>PT6M21S</td>\n",
              "      <td>false</td>\n",
              "      <td>15724</td>\n",
              "      <td>1004</td>\n",
              "      <td>236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UCYjk_zY-iYR8YNfJmuzd70A</td>\n",
              "      <td>Epic Meal Time</td>\n",
              "      <td>562726381</td>\n",
              "      <td>6890000</td>\n",
              "      <td>2010-09-29T19:34:09Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...</td>\n",
              "      <td>428</td>\n",
              "      <td>M5I7kBjPv7o</td>\n",
              "      <td>Never Trust an Influencer | Binge Eater Ep.9</td>\n",
              "      <td>2022-11-21T21:25:58Z</td>\n",
              "      <td>https://i.ytimg.com/vi/M5I7kBjPv7o/default.jpg</td>\n",
              "      <td>PT1H19M17S</td>\n",
              "      <td>false</td>\n",
              "      <td>7508</td>\n",
              "      <td>272</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>UCYjk_zY-iYR8YNfJmuzd70A</td>\n",
              "      <td>Epic Meal Time</td>\n",
              "      <td>562726381</td>\n",
              "      <td>6890000</td>\n",
              "      <td>2010-09-29T19:34:09Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...</td>\n",
              "      <td>428</td>\n",
              "      <td>e9zp3jQy-98</td>\n",
              "      <td>Rejecting $100,000 He Will Never Get Back | Bi...</td>\n",
              "      <td>2022-11-14T21:11:28Z</td>\n",
              "      <td>https://i.ytimg.com/vi/e9zp3jQy-98/default.jpg</td>\n",
              "      <td>PT1H23M14S</td>\n",
              "      <td>false</td>\n",
              "      <td>7891</td>\n",
              "      <td>266</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UCYjk_zY-iYR8YNfJmuzd70A</td>\n",
              "      <td>Epic Meal Time</td>\n",
              "      <td>562726381</td>\n",
              "      <td>6890000</td>\n",
              "      <td>2010-09-29T19:34:09Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-qFeW2GIJmuBxv...</td>\n",
              "      <td>428</td>\n",
              "      <td>Mdn687guoB8</td>\n",
              "      <td>WATCH YOUR LANGUAGE!! #shorts</td>\n",
              "      <td>2022-11-08T16:41:00Z</td>\n",
              "      <td>https://i.ytimg.com/vi/Mdn687guoB8/default.jpg</td>\n",
              "      <td>PT54S</td>\n",
              "      <td>false</td>\n",
              "      <td>23751</td>\n",
              "      <td>1122</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11fb3046-e24d-47b1-a6e2-77a7dc51b350')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11fb3046-e24d-47b1-a6e2-77a7dc51b350 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11fb3046-e24d-47b1-a6e2-77a7dc51b350');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vids_by_channel_id.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnR2QlbclKA3",
        "outputId": "30b8ba45-56c3-477b-b979-2d7c55f7fc6c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100737 entries, 0 to 100736\n",
            "Data columns (total 16 columns):\n",
            " #   Column            Non-Null Count   Dtype \n",
            "---  ------            --------------   ----- \n",
            " 0   chan_id           100737 non-null  object\n",
            " 1   chan_name         100737 non-null  object\n",
            " 2   chan_viewcount    100737 non-null  object\n",
            " 3   chan_subcount     100737 non-null  object\n",
            " 4   chan_start_dt     100737 non-null  object\n",
            " 5   chan_thumb        100737 non-null  object\n",
            " 6   chan_vidcount     100737 non-null  object\n",
            " 7   vid_id            100737 non-null  object\n",
            " 8   vid_name          100737 non-null  object\n",
            " 9   vid_publish_dt    100737 non-null  object\n",
            " 10  vid_thumb         100737 non-null  object\n",
            " 11  vid_duration      100737 non-null  object\n",
            " 12  vid_caption       100737 non-null  object\n",
            " 13  vid_viewcount     100737 non-null  object\n",
            " 14  vid_likecount     100737 non-null  object\n",
            " 15  vid_commentcount  100737 non-null  object\n",
            "dtypes: object(16)\n",
            "memory usage: 13.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method #2: Search Based\n",
        "This method directly taps the YouTube API to collect videos based on a keyword search term."
      ],
      "metadata": {
        "id": "ceyN2RfumW6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search Based Functions"
      ],
      "metadata": {
        "id": "l_dM7_ToqQN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_playlist(yt_client: object, playlist_id: str, max_vids: int = 50) -> List[str]:\n",
        "\n",
        "    # Parameters:\n",
        "    # yt_client -- YouTube API client for requests\n",
        "    # playlist_id -- ID of a YouTube playlist\n",
        "    # max_vids -- Number of videos to return, if available (50 allowed by YouTube)\n",
        "\n",
        "    # Returns:\n",
        "    # List of video ID strings\n",
        "\n",
        "    \n",
        "    # Get initial batch of results\n",
        "\n",
        "    results = yt_client.playlistItems().list(\n",
        "        playlistId = playlist_id,\n",
        "        part = 'snippet',\n",
        "        maxResults = max_vids\n",
        "    ).execute()\n",
        "\n",
        "    video_list = [ video['snippet']['resourceId']['videoId'] for video in results['items'] ]\n",
        "\n",
        "    max_vids = max_vids - 50\n",
        "\n",
        "    while ('nextPageToken' in results) and (max_vids > 0):\n",
        "\n",
        "        # Continue pulling playlist results as long as there is a 'next page'\n",
        "\n",
        "        results = yt_client.playlistItems().list(\n",
        "            part = 'snippet',\n",
        "            playlistId = playlist_id,\n",
        "            pageToken = results['nextPageToken'],\n",
        "            maxResults = max_vids\n",
        "        ).execute()\n",
        "\n",
        "        for video in results['items']:\n",
        "\n",
        "            video_list.append(video['snippet']['resourceId']['videoId'])\n",
        "\n",
        "        max_vids = max_vids - 50\n",
        "        \n",
        "    return video_list        \n",
        "\n",
        "\n",
        "def get_uploads(yt_client: object, channel_id: str, max_vids: int = 50) -> List[str]:\n",
        "\n",
        "    # Finds the most recent uploads associated with a YouTube channel\n",
        "\n",
        "    # Parameters:\n",
        "    # yt_client -- YouTube API client for requests\n",
        "    # channel_id -- ID of a YouTube channel\n",
        "    # max_vids -- Number of videos to return, if available (50 allowed by YouTube)\n",
        "\n",
        "    # Returns:\n",
        "    # List of video ID strings\n",
        "\n",
        "    results = yt_client.channels().list(\n",
        "        part='contentDetails',\n",
        "        id = channel_id,\n",
        "    ).execute()\n",
        "\n",
        "    upload_id = results['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "    upload_list = get_playlist(yt_client, upload_id, max_vids = max_vids)\n",
        "\n",
        "    return upload_list\n",
        "\n",
        "\n",
        "def get_channel_from_vid(yt_client: object, vid_id: str) -> str:\n",
        "\n",
        "    # Finds the channel associated with the input video\n",
        "\n",
        "    # Parameters:\n",
        "    # yt_client -- YouTube API client for requests\n",
        "    # vid_id -- ID of a YouTube video\n",
        "\n",
        "    # Returns:\n",
        "    # ID string of channel associated with video\n",
        "\n",
        "    results = yt_client.videos().list(\n",
        "        part = 'snippet',\n",
        "        id = vid_id\n",
        "    ).execute()\n",
        "\n",
        "    channel_id = results['items'][0]['snippet']['channelId']\n",
        "\n",
        "    return channel_id\n",
        "    \n",
        "\n",
        "\n",
        "def extract_by_query(yt_client: object, query: str, max_channels: int = 50, max_vids: int = 100, excluded_channels: List = []) -> pd.DataFrame:\n",
        "\n",
        "    # Performs data extraction from YouTube API using a keyword(s) query\n",
        "    # Quota estimates use max_channels = 50, max_vids = 100\n",
        "\n",
        "    # Parameters:\n",
        "    # yt_client -- YouTube API client for requests\n",
        "    # query -- A string of key words, presumably related to culinary topics\n",
        "    # max_channels -- the number of channels to survey              \n",
        "    # max_vids -- the number of videos to pull from each channel  \n",
        "    # excluded_channels -- optional list of channel ids to exclude from results (excluded channels are still deducted from the max_channels total)  \n",
        "\n",
        "    # Returns:\n",
        "    # Pandas dataframe with channel and video features\n",
        "\n",
        "    chan_cols = [ 'chan_query', 'chan_id', 'chan_name', 'chan_viewcount', 'chan_subcount', 'chan_start_dt', 'chan_thumb', 'chan_vidcount']\n",
        "    vid_cols = ['vid_id', 'vid_name', 'vid_publish_dt', 'vid_thumb', 'vid_duration', 'vid_caption', 'vid_viewcount', 'vid_likecount', 'vid_commentcount']\n",
        "\n",
        "    df = pd.DataFrame(columns = chan_cols + vid_cols)\n",
        "\n",
        "    channel_results = yt_client.search().list(\n",
        "        part = 'snippet',\n",
        "        type = 'channel',\n",
        "        q = query + ' cooking videos',\n",
        "        maxResults = max_channels\n",
        "    ).execute()\n",
        "\n",
        "    # 100 quota for the search\n",
        "\n",
        "    for channel in channel_results['items']:\n",
        "        channel_id = channel['id']['channelId']\n",
        "\n",
        "        if channel_id in excluded_channels:\n",
        "            continue\n",
        "\n",
        "        chan_info = yt_client.channels().list(\n",
        "            part = ['snippet', 'contentDetails', 'statistics', 'topicDetails'],\n",
        "            id = channel_id\n",
        "        ).execute()['items'][0]\n",
        "\n",
        "        # 1 quota x 50 channels = 50 quota\n",
        "\n",
        "        chan_snip = chan_info['snippet']\n",
        "        chan_det = chan_info['contentDetails']\n",
        "        chan_stats = chan_info['statistics']\n",
        "\n",
        "        # Building dataframe rows, starting with channel features.\n",
        "\n",
        "        chan_values = [ query, channel_id, chan_snip['title'], int(chan_stats['viewCount']), int(chan_stats['subscriberCount']), chan_snip['publishedAt'], chan_snip['thumbnails']['default']['url'], int(chan_stats['videoCount']) ]\n",
        "\n",
        "        chan_uploads_id = chan_det['relatedPlaylists']['uploads']\n",
        "\n",
        "        # Get the id values for the channel's vids\n",
        "        # 2 quota (100 vids = 2 x 50) x 50 channels = 100 quota\n",
        "\n",
        "        # Need to catch upload errors, caused (?) by channels with no videos\n",
        "\n",
        "        try:\n",
        "\n",
        "            vid_ids = get_playlist(yt_client, chan_uploads_id, max_vids)\n",
        "\n",
        "        except Exception:\n",
        "\n",
        "            print(f\"Error retrieving uploads for channel {chan_snip['title']}, ID {channel_id}.\")\n",
        "\n",
        "            continue\n",
        "\n",
        "        for vid_id in vid_ids:\n",
        "\n",
        "            vid_info = yt_client.videos().list(\n",
        "                part = ['contentDetails', 'snippet', 'statistics'],\n",
        "                id = vid_id\n",
        "            ).execute()['items'][0]\n",
        "\n",
        "            # 1 quota x 50 channels x 100 videos = 5000 quota\n",
        "\n",
        "            vid_snip = vid_info['snippet']\n",
        "            vid_det = vid_info['contentDetails']\n",
        "            vid_stats = vid_info['statistics']\n",
        "\n",
        "            # If comments are turned off, the key is missing \n",
        "\n",
        "            if 'commentCount' in vid_stats:\n",
        "                vid_comment_count = int(vid_stats['commentCount'])\n",
        "            else:\n",
        "                vid_comment_count = 0\n",
        "\n",
        "            # Key for likes can be missing \n",
        "\n",
        "            if 'likeCount' in vid_stats:\n",
        "                vid_like_count = int(vid_stats['likeCount'])\n",
        "            else:\n",
        "                vid_like_count = 0\n",
        "\n",
        "            # Key for views can be missing\n",
        "                \n",
        "            if 'viewCount' in vid_stats:\n",
        "                vid_view_count = int(vid_stats['viewCount'])\n",
        "            else:\n",
        "                vid_view_count = 0\n",
        "\n",
        "            # Finish building rows, add to dataframe\n",
        "\n",
        "            vid_values = [ vid_id, vid_snip['title'], vid_snip['publishedAt'], vid_snip['thumbnails']['default']['url'], \n",
        "                            vid_det['duration'], vid_det['caption'], vid_view_count, vid_like_count, vid_comment_count]\n",
        "\n",
        "            current_row = len(df.index)+1\n",
        "\n",
        "            df.loc[current_row,:] = chan_values + vid_values\n",
        "\n",
        "\n",
        "    # Total quota estimate: 100 + 50 + 100 + 5000 = 5250\n",
        "\n",
        "    return df\n",
        "\n",
        "def extract_query(api_key:str, query:str, excluded_chanels:set, compiled_data=None):\n",
        "    \"\"\"extracts a dataframe of query results\n",
        "    Args:\n",
        "        api_key (str): api key as a string\n",
        "        query (str): the query for the search\n",
        "        excluded_chanels (set): a set of channel ids to exclude from the results\n",
        "        compiled_data (None or pd.DataFrame, optional): data to append results to. Defaults to None.\n",
        "    Returns:\n",
        "        tuple(pd.DataFrame, set): updated dataframe with new query results, updated set of excluded channels\n",
        "    \"\"\"\n",
        "    client = make_client(api_key)\n",
        "    query = query.replace('/', ' ')\n",
        "    print('Searching query:', query.rstrip('\\n'))\n",
        "    df = extract_by_query(client, query.rstrip('\\n'), excluded_channels=list(excluded_chanels), max_channels=50, max_vids=50)\n",
        "    if compiled_data is None:\n",
        "        compiled_data = df\n",
        "    else:\n",
        "        compiled_data = pd.concat((compiled_data, df), axis=0, ignore_index=True)\n",
        "    exclude = set(df['chan_id'].unique())\n",
        "    excluded_chanels = excluded_chanels.union(exclude)\n",
        "    return compiled_data, excluded_chanels\n",
        "\n",
        "def api_gen(apis:list):\n",
        "    \"\"\"_summary_\n",
        "    Args:\n",
        "        apis (list): list of api keys as strings\n",
        "    Yields:\n",
        "        str: api key\n",
        "    \"\"\"\n",
        "    for api in apis:\n",
        "        yield api\n",
        "\n",
        "def extract_all(api_key_list:list, query_list:list, excluded_chanels: set, compiled_data=None, with_terminal=False, intermediate_save_folder=None):\n",
        "    \"\"\"extracts queries based on a query list and a list of api keys\n",
        "    Args:\n",
        "        api_key_list (list): list of api keys as strings\n",
        "        query_list (list): a list of querries\n",
        "        excluded_chanels (set): a set of channel ids to exclude from the search results\n",
        "        compiled_data (None or pd.DataFrame, optional): data to append results to. Defaults to None.\n",
        "        with_terminal (bool, optional): whether to inspect each query and decide to modify or skip only for use with a terminal. Defaults to False.\n",
        "        intermediate_save_folder (None, or str, optional): folder to save results after each query. Defaults to None.\n",
        "    Returns:\n",
        "        tuple(pd.DataFrame, set): updated dataframe with new query results, updated set of excluded channels\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    api_keys = api_gen(api_key_list)\n",
        "    api_string = next(api_keys)\n",
        "    for query in query_list:\n",
        "\n",
        "        \n",
        "        if with_terminal:\n",
        "            query = get_input(query.rstrip('\\n').replace('/', ' '))\n",
        "            if query is None:\n",
        "                continue\n",
        "        \n",
        "        try:\n",
        "            compiled_data, excluded_chanels = extract_query(api_string, query=query, excluded_chanels=excluded_chanels, compiled_data=compiled_data)\n",
        "            print('Data Shape:', compiled_data.shape, 'Excluded Channel List len:', len(excluded_chanels))\n",
        "        except HttpError:\n",
        "            try:\n",
        "                api_string = next(api_keys)\n",
        "                print('='*50)\n",
        "                print('Query limit reached trying next api key')\n",
        "                compiled_data, excluded_chanels = extract_query(api_string, query=query, excluded_chanels=excluded_chanels, compiled_data=compiled_data)\n",
        "                print('Data Shape:', compiled_data.shape, 'Excluded Channel List len:', len(excluded_chanels))\n",
        "            except StopIteration:\n",
        "                print(f'Final api ran out on query: {query}, not included in data')\n",
        "                return compiled_data, excluded_chanels\n",
        "        \n",
        "        if intermediate_save_folder is not None:\n",
        "            compiled_data.to_csv(os.path.join(intermediate_save_folder, f'compiled_data_through_query_{query}.csv'), index=False)\n",
        "            excluded = pd.Series(list(excluded_chanels))\n",
        "            excluded.to_csv(os.path.join(intermediate_save_folder, f'excluded_channels_through_query_{query}.csv'), index=False)\n",
        "            \n",
        "\n",
        "\n",
        "    return compiled_data, excluded_chanels\n"
      ],
      "metadata": {
        "id": "gV-vM6UylvAS"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Videos"
      ],
      "metadata": {
        "id": "6H5EwG8utAJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this method, we used a list of search terms related to cooking. \n",
        "# For this demo, we will use 4 example words\n",
        "query_list=['biscuits','snickerdoodle','bbq','muffins']"
      ],
      "metadata": {
        "id": "J3FgzC4yqYFN"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# excluded channels as a set\n",
        "excluded_channels = set('')\n",
        "\n",
        "# data so far if starting from scratch set to None\n",
        "data_file = ''\n",
        "data = pd.read_csv(data_file) if data_file != '' else None\n",
        "\n",
        "# where to save intermediate results (after each query) if blank intermediate saves will not happen\n",
        "intermediate_save_folder = ''\n",
        "\n",
        "# extract data\n",
        "finished_data, excluded_channel_set = extract_all(api_list, query_list, excluded_channels, data, with_terminal=False, intermediate_save_folder=intermediate_save_folder if intermediate_save_folder != '' else None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e94VhwbTuok3",
        "outputId": "5427e9cf-3d83-48fa-a763-784947f72460"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Query limit reached trying next api key\n",
            "Searching query: biscuits\n",
            "Data Shape: (2461, 17) Excluded Channel List len: 50\n",
            "Searching query: snickerdoodle\n",
            "Error retrieving uploads for channel Hi, ID UCDSRBKDbyowqZQimprxLHcg.\n",
            "Data Shape: (4031, 17) Excluded Channel List len: 98\n",
            "Searching query: bbq\n",
            "Data Shape: (6308, 17) Excluded Channel List len: 147\n",
            "Searching query: muffins\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"quotaExceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Query limit reached trying next api key\n",
            "Searching query: muffins\n",
            "Data Shape: (8028, 17) Excluded Channel List len: 190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finished_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "5-nm_ml2xWmI",
        "outputId": "790c7ddd-ee62-4c59-9b23-2e2792ae0dd4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  chan_query                   chan_id          chan_name chan_viewcount  \\\n",
              "0   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "1   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "2   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "3   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "4   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "\n",
              "  chan_subcount         chan_start_dt  \\\n",
              "0       4640000  2015-05-18T04:00:01Z   \n",
              "1       4640000  2015-05-18T04:00:01Z   \n",
              "2       4640000  2015-05-18T04:00:01Z   \n",
              "3       4640000  2015-05-18T04:00:01Z   \n",
              "4       4640000  2015-05-18T04:00:01Z   \n",
              "\n",
              "                                          chan_thumb chan_vidcount  \\\n",
              "0  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "1  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "2  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "3  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "4  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "\n",
              "        vid_id                                           vid_name  \\\n",
              "0  aJ1RVfhLflA  흑설탕 파운드 케이크 만들기 : Dark Brown Sugar Pound Cake ...   \n",
              "1  bJXcZ7WcPtc  #83 베이킹 영상 3배속으로 몰아보기 : 3x Speed Baking Video ...   \n",
              "2  4Xr_8g96dAI  한입 와앙~😚 베어 물고 싶은 도지마롤 * 생크림 롤케이크 만들기 : Dojima ...   \n",
              "3  oSsTQe4Nnxc  킨더 초콜릿 우유 쿠키 만들기 : Kinder Chocolate Milk Cooki...   \n",
              "4  ZiLnr6QWr7A  사 먹는 것보다 맛있어요!👍🏻 쫄깃한 꽈배기 만들기 : Chewy Twisted D...   \n",
              "\n",
              "         vid_publish_dt                                       vid_thumb  \\\n",
              "0  2022-11-29T12:00:00Z  https://i.ytimg.com/vi/aJ1RVfhLflA/default.jpg   \n",
              "1  2022-11-27T04:30:10Z  https://i.ytimg.com/vi/bJXcZ7WcPtc/default.jpg   \n",
              "2  2022-11-26T04:30:01Z  https://i.ytimg.com/vi/4Xr_8g96dAI/default.jpg   \n",
              "3  2022-11-24T12:00:37Z  https://i.ytimg.com/vi/oSsTQe4Nnxc/default.jpg   \n",
              "4  2022-11-22T12:00:21Z  https://i.ytimg.com/vi/ZiLnr6QWr7A/default.jpg   \n",
              "\n",
              "  vid_duration vid_caption vid_viewcount vid_likecount vid_commentcount  \n",
              "0      PT4M33S        true          8157           809               23  \n",
              "1     PT12M23S       false         16416           787               15  \n",
              "2       PT7M3S        true         35525          2246               39  \n",
              "3         PT4M        true         30866          1851               50  \n",
              "4      PT6M12S        true         34104          2052               44  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c379c308-a9fa-4885-b2de-b8768c9ac73a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chan_query</th>\n",
              "      <th>chan_id</th>\n",
              "      <th>chan_name</th>\n",
              "      <th>chan_viewcount</th>\n",
              "      <th>chan_subcount</th>\n",
              "      <th>chan_start_dt</th>\n",
              "      <th>chan_thumb</th>\n",
              "      <th>chan_vidcount</th>\n",
              "      <th>vid_id</th>\n",
              "      <th>vid_name</th>\n",
              "      <th>vid_publish_dt</th>\n",
              "      <th>vid_thumb</th>\n",
              "      <th>vid_duration</th>\n",
              "      <th>vid_caption</th>\n",
              "      <th>vid_viewcount</th>\n",
              "      <th>vid_likecount</th>\n",
              "      <th>vid_commentcount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>aJ1RVfhLflA</td>\n",
              "      <td>흑설탕 파운드 케이크 만들기 : Dark Brown Sugar Pound Cake ...</td>\n",
              "      <td>2022-11-29T12:00:00Z</td>\n",
              "      <td>https://i.ytimg.com/vi/aJ1RVfhLflA/default.jpg</td>\n",
              "      <td>PT4M33S</td>\n",
              "      <td>true</td>\n",
              "      <td>8157</td>\n",
              "      <td>809</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>bJXcZ7WcPtc</td>\n",
              "      <td>#83 베이킹 영상 3배속으로 몰아보기 : 3x Speed Baking Video ...</td>\n",
              "      <td>2022-11-27T04:30:10Z</td>\n",
              "      <td>https://i.ytimg.com/vi/bJXcZ7WcPtc/default.jpg</td>\n",
              "      <td>PT12M23S</td>\n",
              "      <td>false</td>\n",
              "      <td>16416</td>\n",
              "      <td>787</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>4Xr_8g96dAI</td>\n",
              "      <td>한입 와앙~😚 베어 물고 싶은 도지마롤 * 생크림 롤케이크 만들기 : Dojima ...</td>\n",
              "      <td>2022-11-26T04:30:01Z</td>\n",
              "      <td>https://i.ytimg.com/vi/4Xr_8g96dAI/default.jpg</td>\n",
              "      <td>PT7M3S</td>\n",
              "      <td>true</td>\n",
              "      <td>35525</td>\n",
              "      <td>2246</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>oSsTQe4Nnxc</td>\n",
              "      <td>킨더 초콜릿 우유 쿠키 만들기 : Kinder Chocolate Milk Cooki...</td>\n",
              "      <td>2022-11-24T12:00:37Z</td>\n",
              "      <td>https://i.ytimg.com/vi/oSsTQe4Nnxc/default.jpg</td>\n",
              "      <td>PT4M</td>\n",
              "      <td>true</td>\n",
              "      <td>30866</td>\n",
              "      <td>1851</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>ZiLnr6QWr7A</td>\n",
              "      <td>사 먹는 것보다 맛있어요!👍🏻 쫄깃한 꽈배기 만들기 : Chewy Twisted D...</td>\n",
              "      <td>2022-11-22T12:00:21Z</td>\n",
              "      <td>https://i.ytimg.com/vi/ZiLnr6QWr7A/default.jpg</td>\n",
              "      <td>PT6M12S</td>\n",
              "      <td>true</td>\n",
              "      <td>34104</td>\n",
              "      <td>2052</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c379c308-a9fa-4885-b2de-b8768c9ac73a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c379c308-a9fa-4885-b2de-b8768c9ac73a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c379c308-a9fa-4885-b2de-b8768c9ac73a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finished_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDAjiTa6vF8G",
        "outputId": "7e224bff-baf9-4f69-8547-e581dcf5224d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8028 entries, 0 to 8027\n",
            "Data columns (total 17 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   chan_query        8028 non-null   object\n",
            " 1   chan_id           8028 non-null   object\n",
            " 2   chan_name         8028 non-null   object\n",
            " 3   chan_viewcount    8028 non-null   object\n",
            " 4   chan_subcount     8028 non-null   object\n",
            " 5   chan_start_dt     8028 non-null   object\n",
            " 6   chan_thumb        8028 non-null   object\n",
            " 7   chan_vidcount     8028 non-null   object\n",
            " 8   vid_id            8028 non-null   object\n",
            " 9   vid_name          8028 non-null   object\n",
            " 10  vid_publish_dt    8028 non-null   object\n",
            " 11  vid_thumb         8028 non-null   object\n",
            " 12  vid_duration      8028 non-null   object\n",
            " 13  vid_caption       8028 non-null   object\n",
            " 14  vid_viewcount     8028 non-null   object\n",
            " 15  vid_likecount     8028 non-null   object\n",
            " 16  vid_commentcount  8028 non-null   object\n",
            "dtypes: object(17)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Datasets and Clean\n",
        "We need to remove duplicate videos based on the video ID, videos that are shorter than 60 seconds, and videos that contain the word '#shorts' in the title. For our project, we made the decision to attempt to remove as many videos that may be Shorts due to how this video type is new and inherently different from a standard YouTube video. Future research would be needed to create a model that caters to both standard, long-form videos and short-form Shorts."
      ],
      "metadata": {
        "id": "4fTH5fNkxjI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Function"
      ],
      "metadata": {
        "id": "HxJb0N7-yhhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unqualified_videos(df):\n",
        "\n",
        "    # data cleaning for videos\n",
        "    # remove videos<60 seconds and any that contain '#shorts' in the title\n",
        "    # also drop duplicates based on the vid_id\n",
        "\n",
        "    #parameters: dataframe that you are looking to clean\n",
        "\n",
        "\n",
        "    df = df[df['vid_duration'].notna()]\n",
        "    df = df.drop_duplicates(subset='vid_id', keep=\"first\")\n",
        "    df = df[~df['vid_name'].str.contains('#shorts')]\n",
        "    df['vid_seconds'] = df['vid_duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())\n",
        "    df = df[df['vid_seconds']>60]\n",
        "    return df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TqVbl2SJxm0S"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the Combined Dataset"
      ],
      "metadata": {
        "id": "ScSsH9JcyscU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([finished_data,vids_by_channel_id])"
      ],
      "metadata": {
        "id": "2OZ5Rh0AyqNA"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the rows from the scraping method, we fill the chan_query column with 'no query' or whatever is desired\n",
        "final_df['chan_query'] = final_df['chan_query'].fillna('no query')"
      ],
      "metadata": {
        "id": "1ZSWWI-Fy6YI"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izAIed45zV9v",
        "outputId": "6935a1a8-f784-4fb7-b17c-f97ad4b15791"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(108765, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_vid_df = remove_unqualified_videos(final_df)"
      ],
      "metadata": {
        "id": "7XpLPXryzw6h"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_vid_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kvbGIhz8Ke",
        "outputId": "002adf6e-7a8a-468c-f1b9-5a307da4e238"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56093, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_vid_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "sJGZvyExz-1p",
        "outputId": "74b82683-888d-4a1a-e1cd-b304991bfad9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  chan_query                   chan_id          chan_name chan_viewcount  \\\n",
              "0   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "1   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "2   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "3   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "4   biscuits  UCtby6rJtBGgUm-2oD_E7bzw  Cooking tree 쿠킹트리      462890217   \n",
              "\n",
              "  chan_subcount         chan_start_dt  \\\n",
              "0       4640000  2015-05-18T04:00:01Z   \n",
              "1       4640000  2015-05-18T04:00:01Z   \n",
              "2       4640000  2015-05-18T04:00:01Z   \n",
              "3       4640000  2015-05-18T04:00:01Z   \n",
              "4       4640000  2015-05-18T04:00:01Z   \n",
              "\n",
              "                                          chan_thumb chan_vidcount  \\\n",
              "0  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "1  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "2  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "3  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "4  https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...          1381   \n",
              "\n",
              "        vid_id                                           vid_name  \\\n",
              "0  aJ1RVfhLflA  흑설탕 파운드 케이크 만들기 : Dark Brown Sugar Pound Cake ...   \n",
              "1  bJXcZ7WcPtc  #83 베이킹 영상 3배속으로 몰아보기 : 3x Speed Baking Video ...   \n",
              "2  4Xr_8g96dAI  한입 와앙~😚 베어 물고 싶은 도지마롤 * 생크림 롤케이크 만들기 : Dojima ...   \n",
              "3  oSsTQe4Nnxc  킨더 초콜릿 우유 쿠키 만들기 : Kinder Chocolate Milk Cooki...   \n",
              "4  ZiLnr6QWr7A  사 먹는 것보다 맛있어요!👍🏻 쫄깃한 꽈배기 만들기 : Chewy Twisted D...   \n",
              "\n",
              "         vid_publish_dt                                       vid_thumb  \\\n",
              "0  2022-11-29T12:00:00Z  https://i.ytimg.com/vi/aJ1RVfhLflA/default.jpg   \n",
              "1  2022-11-27T04:30:10Z  https://i.ytimg.com/vi/bJXcZ7WcPtc/default.jpg   \n",
              "2  2022-11-26T04:30:01Z  https://i.ytimg.com/vi/4Xr_8g96dAI/default.jpg   \n",
              "3  2022-11-24T12:00:37Z  https://i.ytimg.com/vi/oSsTQe4Nnxc/default.jpg   \n",
              "4  2022-11-22T12:00:21Z  https://i.ytimg.com/vi/ZiLnr6QWr7A/default.jpg   \n",
              "\n",
              "  vid_duration vid_caption vid_viewcount vid_likecount vid_commentcount  \\\n",
              "0      PT4M33S        true          8157           809               23   \n",
              "1     PT12M23S       false         16416           787               15   \n",
              "2       PT7M3S        true         35525          2246               39   \n",
              "3         PT4M        true         30866          1851               50   \n",
              "4      PT6M12S        true         34104          2052               44   \n",
              "\n",
              "   vid_seconds  \n",
              "0        273.0  \n",
              "1        743.0  \n",
              "2        423.0  \n",
              "3        240.0  \n",
              "4        372.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92d2a9b4-4344-4936-b7d8-5f9b0fd88899\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chan_query</th>\n",
              "      <th>chan_id</th>\n",
              "      <th>chan_name</th>\n",
              "      <th>chan_viewcount</th>\n",
              "      <th>chan_subcount</th>\n",
              "      <th>chan_start_dt</th>\n",
              "      <th>chan_thumb</th>\n",
              "      <th>chan_vidcount</th>\n",
              "      <th>vid_id</th>\n",
              "      <th>vid_name</th>\n",
              "      <th>vid_publish_dt</th>\n",
              "      <th>vid_thumb</th>\n",
              "      <th>vid_duration</th>\n",
              "      <th>vid_caption</th>\n",
              "      <th>vid_viewcount</th>\n",
              "      <th>vid_likecount</th>\n",
              "      <th>vid_commentcount</th>\n",
              "      <th>vid_seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>aJ1RVfhLflA</td>\n",
              "      <td>흑설탕 파운드 케이크 만들기 : Dark Brown Sugar Pound Cake ...</td>\n",
              "      <td>2022-11-29T12:00:00Z</td>\n",
              "      <td>https://i.ytimg.com/vi/aJ1RVfhLflA/default.jpg</td>\n",
              "      <td>PT4M33S</td>\n",
              "      <td>true</td>\n",
              "      <td>8157</td>\n",
              "      <td>809</td>\n",
              "      <td>23</td>\n",
              "      <td>273.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>bJXcZ7WcPtc</td>\n",
              "      <td>#83 베이킹 영상 3배속으로 몰아보기 : 3x Speed Baking Video ...</td>\n",
              "      <td>2022-11-27T04:30:10Z</td>\n",
              "      <td>https://i.ytimg.com/vi/bJXcZ7WcPtc/default.jpg</td>\n",
              "      <td>PT12M23S</td>\n",
              "      <td>false</td>\n",
              "      <td>16416</td>\n",
              "      <td>787</td>\n",
              "      <td>15</td>\n",
              "      <td>743.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>4Xr_8g96dAI</td>\n",
              "      <td>한입 와앙~😚 베어 물고 싶은 도지마롤 * 생크림 롤케이크 만들기 : Dojima ...</td>\n",
              "      <td>2022-11-26T04:30:01Z</td>\n",
              "      <td>https://i.ytimg.com/vi/4Xr_8g96dAI/default.jpg</td>\n",
              "      <td>PT7M3S</td>\n",
              "      <td>true</td>\n",
              "      <td>35525</td>\n",
              "      <td>2246</td>\n",
              "      <td>39</td>\n",
              "      <td>423.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>oSsTQe4Nnxc</td>\n",
              "      <td>킨더 초콜릿 우유 쿠키 만들기 : Kinder Chocolate Milk Cooki...</td>\n",
              "      <td>2022-11-24T12:00:37Z</td>\n",
              "      <td>https://i.ytimg.com/vi/oSsTQe4Nnxc/default.jpg</td>\n",
              "      <td>PT4M</td>\n",
              "      <td>true</td>\n",
              "      <td>30866</td>\n",
              "      <td>1851</td>\n",
              "      <td>50</td>\n",
              "      <td>240.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>biscuits</td>\n",
              "      <td>UCtby6rJtBGgUm-2oD_E7bzw</td>\n",
              "      <td>Cooking tree 쿠킹트리</td>\n",
              "      <td>462890217</td>\n",
              "      <td>4640000</td>\n",
              "      <td>2015-05-18T04:00:01Z</td>\n",
              "      <td>https://yt3.ggpht.com/ytc/AMLnZu-raDpPaw-svdkR...</td>\n",
              "      <td>1381</td>\n",
              "      <td>ZiLnr6QWr7A</td>\n",
              "      <td>사 먹는 것보다 맛있어요!👍🏻 쫄깃한 꽈배기 만들기 : Chewy Twisted D...</td>\n",
              "      <td>2022-11-22T12:00:21Z</td>\n",
              "      <td>https://i.ytimg.com/vi/ZiLnr6QWr7A/default.jpg</td>\n",
              "      <td>PT6M12S</td>\n",
              "      <td>true</td>\n",
              "      <td>34104</td>\n",
              "      <td>2052</td>\n",
              "      <td>44</td>\n",
              "      <td>372.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92d2a9b4-4344-4936-b7d8-5f9b0fd88899')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92d2a9b4-4344-4936-b7d8-5f9b0fd88899 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92d2a9b4-4344-4936-b7d8-5f9b0fd88899');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_vid_df.to_csv('demo.csv',index=False)"
      ],
      "metadata": {
        "id": "JlsNuIwB0LUQ"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from this demo, more than half of our rows were dropped due to duplicates and/or shorts. During our project, we were able to collect more than 1 million rows of unique videos, but this took roughly 8 weeks to do so. We typically saw that roughly 10-20% of videos were dropped because they did not meet our requirements; however, this percentage may change as YouTube promotes creators to put out more Shorts over time."
      ],
      "metadata": {
        "id": "M_HF3rjh0Hdi"
      }
    }
  ]
}